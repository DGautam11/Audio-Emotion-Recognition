{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWfSZO1fqTOzHPoY3Q/P5p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DGautam11/Audio-Emotion-Recognition/blob/main/notebooks/01_data_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio Data ETL Pipeline\n",
        "*End-to-end pipeline for ingesting raw audio, normalizing metadata, and serializing tensors for Wav2Vec2 training.*\n"
      ],
      "metadata": {
        "id": "KkS0E_jt-aTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Configuration"
      ],
      "metadata": {
        "id": "0pqtNbyOD4HC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets transformers\n"
      ],
      "metadata": {
        "id": "s2UvES_ISLJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y3Lio6k6J87"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, load_from_disk\n",
        "from transformers import Wav2Vec2Processor\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    IS_COLAB = True\n",
        "    print(\"Running on Google Colab\")\n",
        "\n",
        "    # Colab Paths (Google Drive)\n",
        "    BASE_PATH = \"/content/drive/MyDrive/Datasets/\"\n",
        "    OUTPUT_PATH = \"/content/drive/MyDrive/wav2vec2-processed-data/\"\n",
        "\n",
        "except ImportError:\n",
        "    IS_COLAB = False\n",
        "    print(\"Running Locally\")\n",
        "\n",
        "    # Local Paths (Relative to the notebook)\n",
        "    # Assumes  a 'datasets' folder next to the notebooks folder\n",
        "    BASE_PATH = \"../datasets/\"\n",
        "    OUTPUT_PATH = \"../wav2vec2-processed-data/\"\n",
        "\n",
        "# Create Output Directory\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# Define source directory paths (Dynamic based on BASE_PATH)\n",
        "RAVDESS_PATH = os.path.join(BASE_PATH, \"Ravdess\", \"audio_speech_actors_01-24\") # Adjust subfolders if needed\n",
        "CREMA_PATH = os.path.join(BASE_PATH, \"Crema\")\n",
        "TESS_PATH = os.path.join(BASE_PATH, \"Tess\")\n",
        "SAVEE_PATH = os.path.join(BASE_PATH, \"Savee\")\n",
        "\n",
        "print(f\" Looking for data in: {BASE_PATH}\")\n",
        "print(f\" Processed data will be saved to: {OUTPUT_PATH}\")"
      ],
      "metadata": {
        "id": "1LNaum_w8R8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. PHASE 1: EXTRACTION\n"
      ],
      "metadata": {
        "id": "Zhy5aGle-8K9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Metadata Aggregation (Emotion Dictionaries)\n",
        "\n",
        "*Different datasets use different labeling schemes (e.g., RAVDESS uses \"01\",CREMA uses \"ANG\"). We map all of them to a unified 8-emotion schema (neutral, calm, happy, sad, angry, fearful, disgust, surprised).*"
      ],
      "metadata": {
        "id": "KmcICzosACzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "RAVDESS_EMOTIONS = {\n",
        "    \"01\": \"neutral\", \"02\": \"calm\", \"03\": \"happy\", \"04\": \"sad\",\n",
        "    \"05\": \"angry\", \"06\": \"fearful\", \"07\": \"disgust\", \"08\": \"surprised\"\n",
        "}\n",
        "\n",
        "CREMA_EMOTIONS = {\n",
        "    \"SAD\": \"sad\", \"ANG\": \"angry\", \"DIS\": \"disgust\", \"FEA\": \"fearful\",\n",
        "    \"HAP\": \"happy\", \"NEU\": \"neutral\"\n",
        "}\n",
        "\n",
        "SAVEE_EMOTIONS = {\n",
        "    \"a\": \"angry\", \"d\": \"disgust\", \"f\": \"fearful\", \"h\": \"happy\",\n",
        "    \"n\": \"neutral\", \"sa\": \"sad\", \"su\": \"surprised\"\n",
        "}"
      ],
      "metadata": {
        "id": "Z11o8Mbp9RCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Source Traversal Functions\n",
        "*Logic: Walk through directory trees, extract file paths, and decode filenames into labels.*"
      ],
      "metadata": {
        "id": "7IgHleLREuca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create functions to process each dataset\n",
        "\n",
        "def process_ravdess(path):\n",
        "    data = []\n",
        "    for actor_folder in os.listdir(path):\n",
        "            for filename in os.listdir(os.path.join(path, actor_folder)):\n",
        "                if filename.endswith(\".wav\"):\n",
        "                  file_path = os.path.join(path, actor_folder, filename)\n",
        "                  emotion_code = filename.split(\"-\")[2]\n",
        "                  emotion = RAVDESS_EMOTIONS[emotion_code]\n",
        "                  data.append({\"file_path\": file_path, \"emotion\": emotion})\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def process_crema(path):\n",
        "    data = []\n",
        "    for filename in os.listdir(path):\n",
        "        if filename.endswith(\".wav\"):\n",
        "            file_path = os.path.join(path, filename)\n",
        "            emotion_code = filename.split(\"_\")[2]\n",
        "            emotion = CREMA_EMOTIONS[emotion_code]\n",
        "            data.append({\"file_path\": file_path, \"emotion\": emotion})\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def process_tess(path):\n",
        "    data = []\n",
        "    for emotion_folder in os.listdir(path):\n",
        "        emotion = emotion_folder.split(\"_\")[1].lower()\n",
        "        folder_path = os.path.join(path, emotion_folder)\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if filename.endswith(\".wav\"):\n",
        "                file_path = os.path.join(folder_path, filename)\n",
        "                emotion = filename[0].lower()\n",
        "                data.append({\"file_path\": file_path, \"emotion\": emotion})\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def process_savee(path):\n",
        "    data = []\n",
        "    for filename in os.listdir(path):\n",
        "        if filename.endswith(\".wav\"):\n",
        "            file_path = os.path.join(path, filename)\n",
        "            emotion_code = filename.split('_')[1][0]  # This gets the letter after 'DC_'\n",
        "\n",
        "            # Handle special cases for 'sa' and 'su'\n",
        "            if emotion_code == 's':\n",
        "                emotion_code = filename.split('_')[1][:2]\n",
        "\n",
        "            emotion = SAVEE_EMOTIONS[emotion_code]\n",
        "            data.append({\"file_path\": file_path, \"emotion\": emotion})\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "def process_tess(path):\n",
        "    data = []\n",
        "    for emotion_folder in os.listdir(path):\n",
        "        folder_path = os.path.join(path, emotion_folder)\n",
        "        if os.path.isdir(folder_path):\n",
        "            # Extract emotion from folder name\n",
        "            emotion = emotion_folder.split(\"_\")[-1].lower()\n",
        "\n",
        "            # Special cases for 'fear' and 'surprise'\n",
        "            if emotion == 'fear':\n",
        "                emotion = 'fearful'\n",
        "            elif 'surprise' in emotion:\n",
        "                emotion = 'surprised'\n",
        "\n",
        "            for filename in os.listdir(folder_path):\n",
        "                if filename.endswith(\".wav\"):\n",
        "                    file_path = os.path.join(folder_path, filename)\n",
        "                    data.append({\n",
        "                        \"file_path\": file_path,\n",
        "                        \"emotion\": emotion\n",
        "                    })\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZR56NrE_ExV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eu8c6a6vop36"
      },
      "outputs": [],
      "source": [
        "# Execution: Source Traversal\n",
        "ravdess_df = process_ravdess(RAVDESS_PATH)\n",
        "crema_df = process_crema(CREMA_PATH)\n",
        "tess_df = process_tess(TESS_PATH)\n",
        "savee_df = process_savee(SAVEE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregation: Merge all metadata\n",
        "combined_df = pd.concat([ravdess_df, crema_df, tess_df, savee_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "VL-FZsZbIe7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation: Ensure files exist\n",
        "combined_df[\"status\"] = combined_df[\"file_path\"].apply(lambda path: True if os.path.exists(path) else None)\n",
        "combined_df = combined_df.dropna(subset=[\"status\"]).drop(columns=[\"status\"])\n",
        "combined_df = combined_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "print(f\"Audio Files: {len(combined_df)}\")\n",
        "print(combined_df['emotion'].value_counts())"
      ],
      "metadata": {
        "id": "EQFn_4TxIjIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"Calm\" (192 samples) is too small and similar to \"Neutral\".\n",
        "# Merge them to create a robust, balanced dataset.\n",
        "combined_df['emotion'] = combined_df['emotion'].replace('neutral', 'calm')\n",
        "print(combined_df['emotion'].value_counts())\n"
      ],
      "metadata": {
        "id": "MzfOgFVt_E5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. PHASE 2: TRANSFORMATION (Process)\n",
        " *Normalizing labels and processing audio signals.*"
      ],
      "metadata": {
        "id": "hb48TaKcJUNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Label Encoding"
      ],
      "metadata": {
        "id": "2oJfRQ3MKrp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode emotions as numeric labels\n",
        "label_encoder = LabelEncoder()\n",
        "combined_df['label'] = label_encoder.fit_transform(combined_df['emotion'])\n",
        "\n",
        "id2label = {str(i): label for i, label in enumerate(label_encoder.classes_)}\n",
        "label2id = {label: i for i, label in enumerate(label_encoder.classes_)}\n",
        "\n",
        "# Save to disk as a simple JSON file\n",
        "label_file_path = os.path.join(OUTPUT_PATH, \"label_mapping.json\")\n",
        "\n",
        "with open(label_file_path, \"w\") as f:\n",
        "    json.dump({\"id2label\": id2label, \"label2id\": label2id}, f)\n",
        "\n",
        "print(f\" Label mapping saved to: {label_file_path}\")\n",
        "print(f\"   Mapping: {id2label}\")"
      ],
      "metadata": {
        "id": "V6KKhvAqJa5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save metadata for reproducibility\n",
        "combined_df.to_csv(os.path.join(OUTPUT_PATH,'processed_audio_data.csv'), index=False)"
      ],
      "metadata": {
        "id": "TdJvmkAYLz0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Train and Test Split"
      ],
      "metadata": {
        "id": "kva4sxhNDeNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Splits the processed data into training and testing sets.\n",
        "\n",
        "train_df, test_df = train_test_split(combined_df, test_size=0.2, stratify=combined_df['label'], random_state=42)\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)"
      ],
      "metadata": {
        "id": "XVnu-8KBDEI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Audio Signal Processing"
      ],
      "metadata": {
        "id": "iXArb3H3EOn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')"
      ],
      "metadata": {
        "id": "HeVujPISDx4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(audio):\n",
        "    try:\n",
        "        audio_path = audio['file_path']\n",
        "\n",
        "        speech_array, _ = librosa.load(audio_path, sr=16000, mono=True)\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = processor(\n",
        "            speech_array,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            max_length=32000,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "        input_values = inputs.input_values[0]\n",
        "\n",
        "        # If the processor failed to pad it correctly, DISCARD the file.\n",
        "        if input_values.shape[-1] != 32000:\n",
        "            return None\n",
        "\n",
        "        return {\n",
        "            \"input_values\": input_values,\n",
        "            \"labels\": torch.tensor(audio[\"label\"])\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio['file_path']}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "zy1biPliEqoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# *Applying signal processing and filtering out corrupt audio files.*\n",
        "\n",
        "# Apply Transformation for training set\n",
        "training_set = train_dataset.map(preprocess)\n",
        "# Remove any rows where the processor returned 'None' (corrupt/short files)\n",
        "training_set = training_set.filter(lambda x: x is not None and x.get(\"input_values\") is not None)\n",
        "print(f\" {len(training_set)} valid samples.\")\n",
        "\n",
        "test_set = test_dataset.map(preprocess)\n",
        "test_set = test_set.filter(lambda x: x is not None and x.get(\"input_values\") is not None)\n",
        "print(f\" {len(test_set)} valid samples.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "iIb4tysdItr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. PHASE 3: FORMATTING & SERIALIZATION (Load)\n",
        " *Setting PyTorch format and saving the clean dataset to disk.*"
      ],
      "metadata": {
        "id": "wjpas88bJffo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set Tensor Format\n",
        "# This prepares the data for the PyTorch DataLoader in the next notebook\n",
        "training_set.set_format(type=\"torch\", columns=[\"input_values\", \"labels\"])\n",
        "test_set.set_format(type=\"torch\", columns=[\"input_values\", \"labels\"])\n",
        "\n",
        "#  Save to Disk (Google Drive)\n",
        "print(f\"Saving processed data to: {OUTPUT_PATH}\")\n",
        "\n",
        "training_set.save_to_disk(os.path.join(OUTPUT_PATH, \"train_dataset\"))\n",
        "test_set.save_to_disk(os.path.join(OUTPUT_PATH, \"test_dataset\"))\n",
        "\n",
        "print(\"ETL Pipeline Complete.  Ready for Notebook 02_wav2vec_finetuning.\")"
      ],
      "metadata": {
        "id": "z1mN6v5PP0BW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if files exist\n",
        "print(f\"ðŸ“‚ Checking Output Path: {OUTPUT_PATH}\")\n",
        "print(f\"   - Train Data Exists? {os.path.exists(os.path.join(OUTPUT_PATH, 'train_dataset'))}\")\n",
        "print(f\"   - Test Data Exists?  {os.path.exists(os.path.join(OUTPUT_PATH, 'test_dataset'))}\")\n",
        "print(f\"   - Mapping Exists?    {os.path.exists(os.path.join(OUTPUT_PATH, 'label_mapping.json'))}\")\n",
        "\n",
        "# Check the mapping content\n",
        "with open(os.path.join(OUTPUT_PATH, 'label_mapping.json'), 'r') as f:\n",
        "    data = json.load(f)\n",
        "    print(f\"\\nðŸ”— Final Label Mapping ({len(data['id2label'])} classes):\")\n",
        "    print(data['id2label'])"
      ],
      "metadata": {
        "id": "UFGBeSClAHVi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}